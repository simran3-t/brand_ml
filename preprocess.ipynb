{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import re\n",
    "import ftfy\n",
    "import scrubadub\n",
    "# import string\n",
    "from tabulate import tabulate\n",
    "import demoji\n",
    "# import emoji\n",
    "from nltk.corpus import stopwords\n",
    "import contractions\n",
    "\n",
    "\n",
    "stop = stopwords.words('english')\n",
    "emailDetector = scrubadub.Scrubber(\n",
    "    detector_list=[scrubadub.detectors.EmailDetector])\n",
    "wnl = WordNetLemmatizer()\n",
    "\n",
    "label_codes = {'No': 0, 'Yes': 1}\n",
    "t_handle_regex = r'(^|[^@\\w])@(\\w{1,15})\\b'\n",
    "t_hashtag_regex = r\"#(\\w+)\"\n",
    "t_url_regex = r\"https?://\\S+|www\\.\\S+\"\n",
    "t_markup_regex = r\"<(\\\"[^\\\"]*\\\"|'[^']*'|[^'\\\">])*>\"\n",
    "t_handle_placeholder = ' {{HANDLE}}'\n",
    "t_hashtag_placeholder = ' {{HASHTAG}}'\n",
    "t_url_placeholder = '{{URL}}'\n",
    "t_markup_placeholder = '{{MARKUP}}'\n",
    "emoji_placeholder = '{{EMOJI}}'\n",
    "# domain specific stopwords.\n",
    "stop.extend(['panasonic'])\n",
    "\n",
    "# table = str.maketrans(\"\", \"\")\n",
    "\n",
    "\n",
    "def read_file(path):\n",
    "    df = pd.read_csv(path)\n",
    "    df = df[['text', 'Complaint']]\n",
    "    return df\n",
    "\n",
    "def penn_to_wn(tag):\n",
    "    def is_noun(tag):\n",
    "        return tag in ['NN', 'NNS', 'NNP', 'NNPS']\n",
    "\n",
    "    def is_verb(tag):\n",
    "        return tag in ['VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ']\n",
    "\n",
    "    def is_adverb(tag):\n",
    "        return tag in ['RB', 'RBR', 'RBS']\n",
    "\n",
    "    def is_adjective(tag):\n",
    "        return tag in ['JJ', 'JJR', 'JJS']\n",
    "\n",
    "    # Pos tags to wn tags\n",
    "    if is_adjective(tag):\n",
    "        return wn.ADJ\n",
    "    elif is_noun(tag):\n",
    "        return wn.NOUN\n",
    "    elif is_adverb(tag):\n",
    "        return wn.ADV\n",
    "    elif is_verb(tag):\n",
    "        return wn.VERB\n",
    "    return None\n",
    "\n",
    "\n",
    "def to_lower_case(text):\n",
    "    return text.lower()\n",
    "\n",
    "\n",
    "def fix_unicode(text):\n",
    "    return ftfy.fix_text(text)\n",
    "\n",
    "\n",
    "def replace_email(text):\n",
    "    return emailDetector.clean(text)\n",
    "\n",
    "\n",
    "def remove_stop_words(text):\n",
    "    return ' '.join([word for word in text.split() if word not in (stop)])\n",
    "\n",
    "\n",
    "def convert_emoji_to_text(text):\n",
    "    return text\n",
    "\n",
    "\n",
    "def replace_user_name(text):\n",
    "    return re.sub(t_handle_regex, t_handle_placeholder, text)\n",
    "\n",
    "\n",
    "def replace_hashtags(text):\n",
    "    return re.sub(t_hashtag_regex, t_hashtag_placeholder, text)\n",
    "\n",
    "\n",
    "def replace_url(text):\n",
    "    return re.sub(t_url_regex, t_url_placeholder, text)\n",
    "\n",
    "\n",
    "def replace_markup(text):\n",
    "    return re.sub(t_markup_regex, t_markup_placeholder, text)\n",
    "\n",
    "\n",
    "def remove_punctuations(text):\n",
    "    return re.sub(r'[^\\w\\s]', '', text)\n",
    "\n",
    "\n",
    "def replace_emoji_with_code(text):\n",
    "    demoji.replace(text, repl=emoji_placeholder)\n",
    "    return demoji.replace_with_desc(text)\n",
    "\n",
    "\n",
    "def get_stats(step, df):\n",
    "    corpus = \" \".join(list(df['text']))\n",
    "    total_words = len(corpus.split(' '))\n",
    "    unique_words = len(set(corpus.split(' ')))\n",
    "    return [step, total_words, unique_words]\n",
    "\n",
    "def lemmatize(text):\n",
    "    default_wn_tag = 'n'\n",
    "    tokens = text.split(' ')\n",
    "    pos_tags = nltk.pos_tag(tokens)\n",
    "    wn_tags = [penn_to_wn(tag) for (w, tag) in pos_tags]\n",
    "    # print(list(zip(pos_tags, wn_tags)))\n",
    "    lemmas = [wnl.lemmatize(token, tag or default_wn_tag)\n",
    "              for (token, tag) in list(zip(tokens, wn_tags))]\n",
    "    return ' '.join(lemmas)\n",
    "\n",
    "def fix_contractions(text):\n",
    "    return contractions.fix(text, slang=False)\n",
    "\n",
    "def trim_excessive_space(text):\n",
    "    return \" \".join(text.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Preprocesses the data.\n",
    "\"\"\"\n",
    "def process_data(df, **kwargs):\n",
    "    stats = [['Step', 'Total words', 'Unique words']]\n",
    "    stats.append(get_stats('Start', df))\n",
    "    df = df.replace(label_codes)\n",
    "    df['orig_text'] = df['text']\n",
    "\n",
    "    if kwargs.get('handle_retweet'):\n",
    "        df = df[~df['text'].str.startswith('RT')]\n",
    "        stats.append(get_stats('Remove Retweet', df))\n",
    "    else:\n",
    "        stats.append(['Remove Retweet', 'xxxxxx', 'xxxxxx'])\n",
    "\n",
    "    if kwargs.get('handle_case'):\n",
    "        df['text'] = df['text'].apply(lambda text: text.lower())\n",
    "        stats.append(get_stats('Lower', df))\n",
    "    else:\n",
    "        stats.append(['Lower', 'xxxxxx', 'xxxxxx'])\n",
    "\n",
    "    if kwargs.get('handle_contractions'):\n",
    "        df['text'] = df['text'].apply(lambda text: fix_contractions(text))\n",
    "        stats.append(get_stats('Remove Retweet', df))\n",
    "    else:\n",
    "        stats.append(['Remove Retweet', 'xxxxxx', 'xxxxxx'])\n",
    "\n",
    "\n",
    "    if kwargs.get('handle_lemmatization'):\n",
    "        df['text'] = df['text'].apply(lambda text: lemmatize(text))\n",
    "        stats.append(get_stats('Lemmatize', df))\n",
    "    else:\n",
    "        stats.append(['Lemmatize', 'xxxxxx', 'xxxxxx'])\n",
    "\n",
    "    if kwargs.get('handle_unicode'):\n",
    "        df['text'] = df['text'].apply(fix_unicode)\n",
    "        stats.append(get_stats('Unicode Fix', df))\n",
    "    else:\n",
    "        stats.append(['Unicode Fix', 'xxxxxx', 'xxxxxx'])\n",
    "\n",
    "    if kwargs.get('handle_emoji'):\n",
    "        df['text'] = df['text'].apply(replace_emoji_with_code)\n",
    "        stats.append(get_stats('Replace emoji', df))\n",
    "    else:\n",
    "        stats.append(['Replace emoji', 'xxxxxx', 'xxxxxx'])\n",
    "\n",
    "    if kwargs.get('handle_stopwords'):\n",
    "        df['text'] = df['text'].apply(remove_stop_words)\n",
    "        stats.append(get_stats('Stop words', df))\n",
    "    else:\n",
    "        stats.append(['Stop words', 'xxxxxx', 'xxxxxx'])\n",
    "\n",
    "    if kwargs.get('handle_email'):\n",
    "        df['text'] = df['text'].apply(replace_email)\n",
    "        stats.append(get_stats('Email Replace', df))\n",
    "    else:\n",
    "        stats.append(['Email Replace', 'xxxxxx', 'xxxxxx'])\n",
    "\n",
    "    if kwargs.get('handle_username'):\n",
    "        df['text'] = df['text'].apply(replace_user_name)\n",
    "        stats.append(get_stats('UserName replace', df))\n",
    "    else:\n",
    "        stats.append(['UserName replace', 'xxxxxx', 'xxxxxx'])\n",
    "\n",
    "    if kwargs.get('handle_hashtags'):\n",
    "        df['text'] = df['text'].apply(replace_hashtags)\n",
    "        stats.append(get_stats('HashTags Replace', df))\n",
    "    else:\n",
    "        stats.append(['HashTags Replace', 'xxxxxx', 'xxxxxx'])\n",
    "\n",
    "    if kwargs.get('handle_url'):\n",
    "        df['text'] = df['text'].apply(replace_url)\n",
    "        stats.append(get_stats('URL Replace', df))\n",
    "    else:\n",
    "        stats.append(['URL Replace', 'xxxxxx', 'xxxxxx'])\n",
    "\n",
    "    if kwargs.get('handle_markup'):\n",
    "        df['text'] = df['text'].apply(replace_markup)\n",
    "        stats.append(get_stats('MARKUP Replace', df))\n",
    "    else:\n",
    "        stats.append(['MARKUP Replace', 'xxxxxx', 'xxxxxx'])\n",
    "\n",
    "    if kwargs.get('handle_punctuation'):\n",
    "        df['text'] = df['text'].apply(remove_punctuations)\n",
    "        stats.append(get_stats('Remove punctuation', df))\n",
    "    else:\n",
    "        stats.append(['Remove punctuation', 'xxxxxx', 'xxxxxx'])\n",
    "\n",
    "\n",
    "    df['text'] = df['text'].apply(lambda t: trim_excessive_space(t))\n",
    "    print(tabulate(stats)) if kwargs.get('print_stats', False) else None\n",
    "    return df\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "e490422cd5eb80d883ca3c84d0761f55a6ef3e6322675b57b784cf7be9fc5aec"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 64-bit ('venv_brand_ml': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
